{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a98c5-87b1-480e-9df4-8849a589bcd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9effa-d96f-43c0-8709-da9d85ca4e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "# setting path\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106556c-706b-455a-a972-56c60ae8b8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchio as tio\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from training.segmentation_module import BinarySegmentation\n",
    "from utils.inference_utils import output_name, load_single_slice\n",
    "from utils.config_utils import read_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a2316-0a08-4c2d-a9e0-9a6ff5979938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 12.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7c88f-908b-405f-b8dd-ece077e249c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'yurwvjn0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6736cd-9f94-414e-8f2d-6cefab6c103b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417f0de-f98e-42a2-adfd-31adb334e42c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"../trunk-segmentation/{}/checkpoints/last.ckpt\".format(model_id)\n",
    "data_path = \"../data/scan.tif\"\n",
    "axis = 0\n",
    "idx = 804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b629b-98bf-41f1-a368-3e237f614f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BinarySegmentation.load_from_checkpoint(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ecbc68-2e3f-4a17-a840-5fb72fc4bddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14195f-c929-40f4-97bd-b68a4ab84fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "general_transforms = read_transforms(config['transforms']['general'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806510c-c49f-48f2-9fc7-b67a81a49a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_image = load_single_slice(data_path, idx, axis, as_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f18f1-83c3-4337-9461-4e0cb4d63e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skimage.io.imshow(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc01c9-ff5f-4223-a76a-d60b39d25bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformed_image = general_transforms(image=input_image)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f33f2-89c3-4021-a85d-bee9f5082617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformed_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95deccc4-da6f-4d06-bb3a-b60b09780d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image, mask=None, padding=10):\n",
    "    meansY, meansX = np.mean(image, axis=1), np.mean(image, axis=0)    \n",
    "    selected_indices = np.asarray(meansX > np.round(np.min(meansX))).nonzero()[0]\n",
    "    minX = selected_indices[0] - padding\n",
    "    maxX = selected_indices[-1] + padding\n",
    "    selected_indices = np.asarray(meansY > np.round(np.min(meansY))).nonzero()[0]\n",
    "    minY = selected_indices[0] - padding\n",
    "    maxY = selected_indices[-1] + padding\n",
    "    if mask is not None:\n",
    "        return image[minY:maxY, minX:maxX], mask[minY:maxY, minX:maxX]\n",
    "    return image[minY:maxY, minX:maxX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a3491-219f-4c2a-aa4e-1a3fda0f21e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skimage.io.imshow(crop(transformed_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc856f-7a3d-40e2-bedc-cc103f2b41ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_tensor = torch.from_numpy(transformed_image).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468175f-e7fd-4660-bf56-6b2d71fdb16a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scalar_input_image =tio.ScalarImage(tensor=image_tensor.unsqueeze(0).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a488ce-851a-4929-9c52-58abb698844b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_sampler = tio.inference.GridSampler(\n",
    "        tio.Subject(one_image=scalar_input_image),\n",
    "        (128, 128, 1), #(config['patch_size'], config['patch_size'], 1),  # TODO: when passing a volume we might want to select an axis\n",
    "        (32, 32, 0) #(config['stride'], config['stride'], 0),\n",
    "    )\n",
    "patch_loader = torch.utils.data.DataLoader(grid_sampler, batch_size=config['batch_size'])\n",
    "aggregator = tio.inference.GridAggregator(grid_sampler, overlap_mode='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac4c0c-42e4-430f-8768-f23e1a0770ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for patches_batch in tqdm(patch_loader, desc='Predicting'):\n",
    "        input_tensor = patches_batch['one_image'][tio.DATA].type(torch.FloatTensor).to(device)\n",
    "        locations = patches_batch[tio.LOCATION]\n",
    "        logits = model(input_tensor.squeeze(-1)) # .round()\n",
    "        aggregator.add_batch(logits.unsqueeze(1).unsqueeze(-1), locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51759b-6a2e-42c6-9aa9-db84628f50dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_tensor = aggregator.get_output_tensor()#.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc1662-ff41-4561-bcbd-99b34ebf07d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_mask(img, mask, color=(255, 0, 0), alpha=0.5):\n",
    "  # credits: https://stackoverflow.com/questions/9193603/applying-a-coloured-overlay-to-an-image-in-either-pil-or-imagemagik\n",
    "\n",
    "  img_color = skimage.color.gray2rgb(img)\n",
    "\n",
    "  col_mask = np.ones((mask.shape[0], mask.shape[1], 3)) * (255, 0 ,0)\n",
    "\n",
    "  img_hsv = skimage.color.rgb2hsv(img_color)\n",
    "  color_mask_hsv = skimage.color.rgb2hsv(col_mask)\n",
    "\n",
    "  # Replace the hue and saturation of the original image\n",
    "  # with that of the color mask\n",
    "  img_hsv[..., 0] = color_mask_hsv[..., 0]\n",
    "  img_hsv[..., 1] = color_mask_hsv[..., 1] * mask\n",
    "\n",
    "  return skimage.color.hsv2rgb(img_hsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364afbc8-e8e7-4924-a3ac-34370a4ac377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = output_tensor.squeeze(0).squeeze(-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868cdb0-7f49-4521-a332-d93ff1d6714c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skimage.io.imshow(visualize_mask(transformed_image, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca711d4-d659-4382-a64d-638574700ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (15.0, 12.0) \n",
    "img_cropped, mask_cropped = crop(transformed_image, mask)\n",
    "skimage.io.imshow(visualize_mask(img_cropped, mask_cropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4b379-fc37-4d9d-9985-fdcff2064172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
